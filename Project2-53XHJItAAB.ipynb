{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58fbf49e-f6a3-4be2-a21f-087946ac0595",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#get the file name from the adf\n",
    "fileName='Products.csv'\n",
    "#fileName = dbutils.widgets.get('fileName')\n",
    "fileNameWithoutExt = fileName.split('.')[0]\n",
    "print(fileNameWithoutExt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b87ca8cf-719f-48d4-a059-154ad473f904",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "All the sensitive information has been stored in the key vault. We established the connection with the key vault from the databricks secret scope. And feteching the information using the databricks secrets get method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6259a115-ab8d-46f2-9a36-7bda72962b56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "#from datetime import datetme as dt\n",
    "\n",
    "#Just change all the values here based on the resource name you have created in your environemnt and workspace.\n",
    "#All the key values are assinged to the variables.\n",
    "\n",
    "sqlDbName = 'abdproject '\n",
    "dbUserName = 'abduser'\n",
    "passwordKey = 'adbuserkey'\n",
    "stgAccountSASTokenKey = 'abhinay250898SasToken'\n",
    "landingFileName =fileName #'Product'  #dbutils.widgets.get('Product')\n",
    "databricksScopeName ='ApMorganSecret'\n",
    "dbServer = 'abdserver'\n",
    "dbServerPortNumber ='1433'\n",
    "storageContainer ='apmorgan'\n",
    "storageAccount='abhinay250898adls'\n",
    "landingMountPoint ='/mnt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66acdab5-4499-4721-b02c-50f145958c4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.secrets.get(scope = 'KeyVaultAccess', key= stgAccountSASTokenKey) #KeyVaultAccess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73ad4b87-b6df-4382-a6cf-e8dd9b1d21fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "client_id = dbutils.secrets.get(scope=databricksScopeName, key=\"ClientId\")       # Service Principal App (Client) ID\n",
    "client_secret = dbutils.secrets.get(scope=databricksScopeName, key=\"ClientSecret\")  # Client Secret\n",
    "tenant_id = dbutils.secrets.get(scope=databricksScopeName, key=\"TenantId\")      # Tenant ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8455c24d-5f51-4f37-8116-7fe90b0f7724",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "configs = {\"fs.azure.account.auth.type\": \"OAuth\",\n",
    "          \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "          \"fs.azure.account.oauth2.client.id\": client_id,\n",
    "          \"fs.azure.account.oauth2.client.secret\": client_secret,\n",
    "          \"fs.azure.account.oauth2.client.endpoint\": f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62f86cff-74a4-4b42-847f-37891f1d31c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.mount(source=\"abfss//container@storageaccount.dfs.core.window.net\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1791d71e-246d-4fb9-ab59-9283fc2a0267",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if not any(mount.mountPoint == landingMountPoint for mount in dbutils.fs.mounts()):\n",
    "    dbutils.fs.mount(\n",
    "            source = f\"abfss://{storageContainer}@{storageAccount}.dfs.core.windows.net/\",\n",
    "            mount_point = \"/mnt\",\n",
    "            extra_configs = configs)\n",
    "    print('Storage account created')\n",
    "else:\n",
    "    print('Storage account already mounted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be56aba3-aee9-4567-8590-49cbf7dc6eb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#connect to Azure SQL DB\n",
    "dbPassword = dbutils.secrets.get(scope = databricksScopeName, key= passwordKey)\n",
    "serverurl = 'jdbc:sqlserver://{}.database.windows.net:{};database={};user={};'.format(dbServer, dbServerPortNumber,sqlDbName,dbUserName)\n",
    "connectionProperties = {\n",
    "    'password':dbPassword,\n",
    "    'driver':'com.microsoft.sqlserver.jdbc.SQLServerDriver'\n",
    "}\n",
    "df = spark.read.jdbc(url = serverurl, table = 'dbo.FileDetailsFormat', properties= connectionProperties)\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "638c358e-b149-4675-ad1c-33376b3d542e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1 = spark.read.csv('/mnt/landing/'+fileName, inferSchema=True, header=True)\n",
    "#display(df1)\n",
    "\n",
    "# Rule\n",
    "errorFlag=False\n",
    "errorMessage = ''\n",
    "totalcount = df1.count()\n",
    "print(totalcount)\n",
    "distinctCount = df1.distinct().count()\n",
    "print(distinctCount)\n",
    "if distinctCount !=totalcount:\n",
    "    errorFlag = True\n",
    "    errorMessage = 'Duplication Found. Rule 1 Failed'\n",
    "print(errorMessage)\n",
    "    \n",
    "# Rule 2\n",
    "df2 = df.filter(df.FileName==fileNameWithoutExt).select('ColumnName','ColumnDateFormat' )\n",
    "rows = df2.collect()\n",
    "for r in rows:\n",
    "    colName = r[0]\n",
    "    colFormat =r[1]\n",
    "    print(colName, colFormat)\n",
    "    #display(df1.filter(F.to_date(colName, colFormat).isNull() ==True))\n",
    "    formatCount =df1.filter(F.to_date(colName, colFormat).isNotNull() ==True).count()\n",
    "    if formatCount == totalcount:\n",
    "        errorFlag = True\n",
    "        errorMessage = errorMessage +' DateFormate is incorrect for {} '.format(colName)\n",
    "    else:\n",
    "        print('All rows are good for ', colName)\n",
    "print(errorMessage)\n",
    "\n",
    "if errorFlag:\n",
    "    dbutils.fs.mv('/mnt/landing/'+fileName,'/mnt/rejected/'+fileName )\n",
    "    dbutils.notebook.exit('{\"errorFlag\": \"true\", \"errorMessage\":\"'+errorMessage +'\"}')\n",
    "else:\n",
    "    dbutils.fs.mv('/mnt/landing/'+fileName,'/mnt/staging/'+fileName )\n",
    "    dbutils.notebook.exit('{\"errorFlag\": \"false\", \"errorMessage\":\"No error\"}')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Project2-53XHJItAAB",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
